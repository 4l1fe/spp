# Description       ########################################################################################

Goal to create a packages of functions util_spark.py
where we can do transformation of data, input and output of data
using spark dataframe, RDD and datasets




# Utilities for data processing in spark :  ##############################################################
aapackage/util_spark.py




# Functions are 3 types :
```
X :  RDD OR Dataframe OR Datasets (all 3 )



Input functions :
   Kafka streaming -->  X
   SQL             -->  X



Transformer functions :
      X -->  X 
      X --> Pandas Dataframe
      X --> Scipy Sparse Array

            
            
Output functions :     
   X --> HIVE
   X --> Kafka Streaming
   X --> CSV files
   X --> CouchDB (JSON)

```



#Reauirements   ###########################################################################################
   1 file only  util_spark.py
   Functionnal based coding (do not use class when possible):
   Sample usage code should be provided (using data from csv) in testcode.py
   Please add data in csv for testing.
   Unit tests code with pytest 
   Python 2.7 / 3.0  compatible
   Please commit directly to the folder.




# Infos :   ##############################################################################################
https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html
https://stackoverflow.com/questions/31031597/construct-sparse-matrix-on-disk-on-the-fly-in-python
https://stackoverflow.com/questions/40557577/pyspark-sparse-vectors-to-scipy-sparse-matrix
https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html      



